{
 "metadata": {
  "name": "11_photoz_regression"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Regression Example: Photometric Redshifts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://astroml.github.com/book_figures/chapter9/fig_photoz_tree.html\n",
      "\n",
      "http://astroml.github.com/book_figures/chapter9/fig_photoz_forest.html"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Photometric Redshifts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Blah... show a plot"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Decision Tree Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from datasets import fetch_sdss_galaxy_mags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This will download a ~3MB file the first time you call the function\n",
      "data = fetch_sdss_galaxy_mags()\n",
      "\n",
      "print data.shape\n",
      "print data.dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "redshift = data['redshift']\n",
      "mags = np.vstack([data[f] for f in 'ugriz']).transpose()\n",
      "colors = mags[:, :-1] - mags[:, 1:]\n",
      "print colors.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "ctrain, ctest, ztrain, ztest = cross_validation.train_test_split(colors, redshift)\n",
      "\n",
      "clf = DecisionTreeRegressor()\n",
      "clf.fit(ctrain, ztrain)\n",
      "zpred = clf.predict(ctest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cross_validation.cross_val_score(clf, colors, redshift, cv=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_redshifts(ztest, zpred):\n",
      "    fig, ax = plt.subplots(figsize=(8, 8))\n",
      "    ax.plot(ztest, zpred, '.k')\n",
      "    \n",
      "    # plot trend lines, +/- 0.1 in z\n",
      "    ax.plot([0, 3], [0, 3], '--k')\n",
      "    ax.plot([0, 3], [0.2, 3.2], ':k')\n",
      "    ax.plot([0.2, 3.2], [0, 3], ':k')\n",
      "    \n",
      "    ax.text(2.9, 0.1,\n",
      "            \"RMS = %.2g\" % np.sqrt(np.mean((ztest - zpred) ** 2)),\n",
      "            ha='right', va='bottom')\n",
      "\n",
      "    ax.set_xlim(0, 3)\n",
      "    ax.set_ylim(0, 3)\n",
      "    \n",
      "    ax.set_xlabel('True redshift')\n",
      "    ax.set_ylabel('Predicted redshift')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_redshifts(ztest, zpred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimizing the Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we'll explore results for max_depth from 1 to 20\n",
      "max_depth_array = np.arange(1, 21)\n",
      "train_error = np.zeros(len(max_depth_array))\n",
      "test_error = np.zeros(len(max_depth_array))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, max_depth in enumerate(max_depth_array):\n",
      "    clf = DecisionTreeRegressor(max_depth=max_depth)\n",
      "    clf.fit(ctrain, ztrain)\n",
      "\n",
      "    ztrain_pred = clf.predict(ctrain)\n",
      "    ztest_pred = clf.predict(ctest)\n",
      "\n",
      "    train_error[i] = np.sqrt(np.mean((ztrain_pred - ztrain) ** 2))\n",
      "    test_error[i] = np.sqrt(np.mean((ztest_pred - ztest) ** 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(max_depth_array, train_error)\n",
      "plt.plot(max_depth_array, test_error)\n",
      "plt.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "clf = DecisionTreeRegressor()\n",
      "grid = GridSearchCV(clf, param_grid=dict(max_depth=max_depth_array))\n",
      "grid.fit(colors, redshift)\n",
      "print grid.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = DecisionTreeRegressor(max_depth=10)\n",
      "clf.fit(ctrain, ztrain)\n",
      "zpred = clf.predict(ctest)\n",
      "plot_redshifts(ztest, zpred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def outlier_fraction(y_pred, y_true, cutoff=0.2):\n",
      "    return np.sum((abs(y_pred - y_true) > cutoff)) * 1. / len(y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_outfrac = np.zeros(len(max_depth_array))\n",
      "test_outfrac = np.zeros(len(max_depth_array))\n",
      "\n",
      "for i, max_depth in enumerate(max_depth_array):\n",
      "    clf = DecisionTreeRegressor(max_depth=max_depth)\n",
      "    clf.fit(ctrain, ztrain)\n",
      "\n",
      "    ztrain_pred = clf.predict(ctrain)\n",
      "    ztest_pred = clf.predict(ctest)\n",
      "\n",
      "    train_outfrac[i] = outlier_fraction(ztrain_pred, ztrain)\n",
      "    test_outfrac[i] = outlier_fraction(ztest_pred, ztest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(max_depth_array, train_outfrac)\n",
      "plt.plot(max_depth_array, test_outfrac)\n",
      "plt.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = DecisionTreeRegressor(max_depth=20)\n",
      "clf.fit(ctrain, ztrain)\n",
      "zpred = clf.predict(ctest)\n",
      "plot_redshifts(ztest, zpred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimizing the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do a learning curve thing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise: Better Results Through Ensemble Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One way to improve on Decision Trees is to use *Ensemble Methods*.\n",
      "Ensemble methods (also known as *boosting* and *bagging*) use ensembles\n",
      "of randomized estimators which can prevent over-fitting and lead to very\n",
      "powerful learning algorithms.\n",
      "\n",
      "It is interesting to see how small an RMS you can attain on the photometric\n",
      "redshift problem using a more sophisticated method.  Try one of the following:\n",
      "\n",
      "- ``sklearn.ensemble.RandomForestRegressor``\n",
      "- ``sklearn.ensemble.GradientBoostingRegressor``\n",
      "- ``sklearn.ensemble.ExtraTreesRegressor``\n",
      "\n",
      "You can read more about each of these methods in the scikit-learn documentation:\n",
      "\n",
      "- http://scikit-learn.org/stable/modules/ensemble.html\n",
      "\n",
      "Each method has hyperparameters which need to be determined using cross-validation\n",
      "steps like those above.  Can you use ensemble methods to reduce the rms error for\n",
      "the test set down below 0.1?\n",
      "\n",
      "Here you can adjust several hyperparameters, but the important ones will be\n",
      "the number of estimators ``n_estimators`` as well as the maximum depth\n",
      "``max_depth`` that we saw above."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}